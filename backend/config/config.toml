# iDO Backend default configuration file

# Monitoring configuration
[monitoring]
capture_interval = 1  # seconds
window_size = 60        # seconds
processing_interval = 30 # seconds

# Server configuration
[server]
host = "0.0.0.0"
port = 8000
debug = false

# Logging configuration
[logging]
level = "DEBUG"
logs_dir = "./logs"
max_file_size = "10MB"
backup_count = 5

# Image processing configuration
[image]
compression_quality = 85
max_width = 1920
max_height = 1080
enable_phash = true

# Memory-first storage configuration
enable_memory_first = true          # Master switch
memory_ttl_multiplier = 2.5         # TTL = processing_interval * multiplier
memory_ttl_min = 60                 # Minimum TTL (seconds)
memory_ttl_max = 120                # Maximum TTL (seconds)

# Screenshot configuration
[screenshot]
# Smart capture - only capture the active monitor
# Reduces resource usage in multi-monitor setups by 50-80%
smart_capture_enabled = true

# Inactivity timeout (seconds)
# When no mouse/keyboard activity for this duration, capture all monitors
inactive_timeout = 30

# ============ Image Optimization Configuration - LLM Token Saving ============
#
# The image optimization system reduces LLM API costs through three strategies:
# 1. Smart sampling - Filter duplicate/similar images based on perceptual hashing
# 2. Content analysis - Skip static/blank screens
# 3. Dynamic compression - Intelligently adjust image quality and resolution
#
# Expected savings (based on default configuration):
# - Image count reduction: 60-75% (through sampling and content analysis)
# - Single image size reduction: 70-85% (through dynamic compression)
# - Total token consumption reduction: ~85-95%
#
[image_optimization]

# ========== Basic Switch ==========
# Enable image optimization (master switch)
enabled = true

# Optimization strategy:
# - "none": No optimization, all images sent as-is
# - "sampling": Smart sampling only (based on perceptual hashing)
# - "content_aware": Content-aware filtering only
# - "hybrid": Hybrid optimization (recommended, combines sampling and content analysis)
strategy = "hybrid"

# ========== Smart Sampling Configuration ==========
# Perceptual hash change detection threshold (0.0-1.0)
# Description: Compare similarity of current image with previous one, keep only if exceeds threshold
# - 0.05-0.10: Ultra aggressive (saves 80-90%, may lose details)
# - 0.10-0.15: Aggressive (saves 60-75%, suitable for most scenarios) ⭐ Recommended
# - 0.15-0.20: Balanced (saves 40-60%, retains more details)
# - 0.20-0.30: Conservative (saves 20-40%, maximizes information retention)
phash_threshold = 0.15

# Minimum sampling interval (seconds)
# Description: Time interval between sampled images (only used if filtered images > max_images_per_event)
# With action_extraction_threshold=12, typical span is ~12 seconds
# To get 8 images from 12 seconds: 12/8 = 1.5s interval
# - 1.0-1.5: High frequency sampling (suitable for capturing rapid changes)
# - 1.5-2.0: Balanced sampling (recommended, captures key moments) ⭐ Recommended
# - 2.0-3.0: Low frequency sampling (may miss some transitions)
min_sampling_interval = 1.5  # Optimized: balanced for ~12 second span

# Maximum number of images to sample per event sequence
# Description: Maximum number of images included in a single summary (hard limit)
# - 3-5: Minimal mode (significant cost savings, suitable for simple tasks)
# - 6-8: Standard mode (recommended, balances information and cost) ⭐ Recommended
# - 10-15: Rich mode (more context, suitable for complex tasks)
# Note: This is controlled by processing.max_screenshots_per_extraction instead

# ========== Content Analysis Configuration ==========
# Enable content analysis
# Description: Detect image contrast, edge activity, motion, etc.
# Can additionally skip 10-20% of static/blank content
enable_content_analysis = true

# Enable text detection (experimental feature)
# Description: Use OCR to detect text changes in images
# Note: Requires pytesseract and tesseract-ocr installation
# Significantly increases processing time (+0.5-2 seconds per image)
enable_text_detection = false

# ========== Dynamic Image Compression Configuration ==========
# Compression level (affects quality and resolution)
# - ultra: Maximum compression
#   Quality 30-50, Resolution 400x300-600x400
#   Tokens/image: ~50-80, Suitable for: Text-only interfaces, quick preview
#
# - aggressive: Aggressive compression ⭐ Recommended
#   Quality 40-60, Resolution 480x360-800x600
#   Tokens/image: ~80-120, Suitable for: Most scenarios, general office work
#
# - balanced: Balanced compression
#   Quality 55-75, Resolution 800x450-1280x720
#   Tokens/image: ~150-200, Suitable for: Design work, UI details
#
# - quality: Quality priority
#   Quality 75-85, Resolution 1280x720-1920x1080
#   Tokens/image: ~250-350, Suitable for: High-precision tasks, debugging
compression_level = "aggressive"

# Enable smart region cropping
# Description: Detect changing regions in images, keep only changed parts
# Effect: Can reduce image size by additional 30-60%
# Note: Increases CPU overhead (+50-200ms per image)
# Recommendation: Enable only when image changes are concentrated in local areas (e.g., editors)
enable_region_cropping = false

# Region cropping difference threshold (0-255)
# Description: Pixel brightness difference must exceed this value to be considered a changed region
# - 10-20: Sensitive (retains more regions)
# - 20-40: Balanced (recommended) ⭐
# - 40-60: Strict (retains only obvious changes)
crop_threshold = 30

# ========== Performance Parameters ==========
# Memory cache size (images)
# Description: Cache recent image base64 data in memory
# Recommendation: 200-500 (memory usage ~100-250MB)
memory_cache_size = 500

# ========== Optimization Effect Estimation ==========
# Based on default configuration (aggressive + hybrid), with 20 original screenshots as example:
#
# 1. Smart sampling (phash_threshold=0.15, min_interval=2.0)
#    20 images → 6-8 images (reduce 60-70%)
#
# 2. Content analysis (skip static screens)
#    6-8 images → 5-6 images (additional 10-20% reduction)
#
# 3. Dynamic compression (aggressive, 800x600, quality 50)
#    Each ~250KB → 40KB (reduce 84%)
#    Each ~350 tokens → 100 tokens (reduce 71%)
#
# Overall effect:
# - Image count: 20 → 5-6 images (reduce 70-75%)
# - Token consumption: 7000 → 500-600 (reduce 91-93%)
# - Single summary cost (GPT-4V): ~$0.070 → $0.006 (reduce 91%)

# ============ New Architecture Configuration ============

# Processing configuration
[processing]
# Perception layer configuration (screenshot_interval controlled by monitoring.capture_interval)
enable_screenshot_deduplication = true  # Enable screenshot deduplication

# ========== Session Agent Merging Configuration ==========
# Time gap tolerance for merging adjacent activities (seconds)
# Description: Maximum time gap between activities to consider merging if they are semantically similar
# - 60-180: Conservative (only merge very close activities, 1-3 minutes)
# - 180-300: Standard (recommended, suitable for most scenarios) ⭐
# - 300-600: Aggressive (merge activities with larger gaps, 5-10 minutes)
merge_time_gap_tolerance = 300

# Semantic similarity threshold for merging (0.0-1.0)
# Description: Minimum similarity score required to merge adjacent activities
# - 0.8-1.0: Strict (only merge nearly identical activities)
# - 0.6-0.8: Standard (recommended, balances accuracy and merging) ⭐
# - 0.4-0.6: Relaxed (more aggressive merging, may merge loosely related activities)
merge_similarity_threshold = 0.6

# ========== Advanced Screenshot Deduplication Configuration ==========
# Similarity threshold (0.0-1.0)
# Description: Two screenshots are considered duplicates if similarity exceeds this value
# - 0.85-0.90: Relaxed mode (retains more screenshots, suitable for fast-changing scenarios)
# - 0.90-0.95: Standard mode (recommended, balances deduplication and information retention) ⭐
# - 0.95-0.98: Strict mode (aggressive deduplication, suitable for static content scenarios)
screenshot_similarity_threshold = 0.92  # Optimized: increased from 0.90 for more aggressive deduplication

# Hash cache size
# Description: Keep hash values of the last N screenshots for comparison
# - 5-10: Standard mode (recommended, suitable for most scenarios) ⭐
# - 10-20: Enhanced mode (can detect older duplicates, slightly higher CPU overhead)
# - 1-3: Lightweight mode (only compare with recent images, optimal performance)
screenshot_hash_cache_size = 10

# Hash algorithm list
# Description: Use multiple algorithms to improve detection accuracy
# Available algorithms: phash (perceptual hash), dhash (difference hash), average_hash (average hash)
# Recommendation: Keep default configuration (three algorithms combined)
screenshot_hash_algorithms = ["phash", "dhash", "average_hash"]

# Scene adaptive threshold
# Description: Automatically adjust similarity threshold based on content type
# - true: Enable (recommended) - video scenes retain key frames, static scenes use aggressive deduplication
# - false: Disable - always use fixed threshold
enable_adaptive_threshold = true

# Action extraction configuration (RawAgent → ActionAgent flow)
# Threshold calculation: target 8 screenshots after filtering
# Typical filtering rate: 30-40% (dedup + content analysis)
# Formula: threshold = max_screenshots / (1 - filter_rate) ≈ 8 / 0.65 ≈ 12
action_extraction_threshold = 12  # Optimized: trigger when 12 screenshots accumulated (~12 seconds)
max_screenshots_per_extraction = 8  # Final limit: ImageSampler will select best 8 from filtered results

# UI configuration
[ui]
recent_events_count = 5  # Number of recent events to display (can be modified in frontend)

# Language configuration
[language]
# System language configuration, affects prompt selection
default_language = "zh"  # zh | en

# Database configuration
[database]
# Database file path (relative to data directory or absolute path)
# Leave empty to use default path: ~/.config/ido/ido.db
path = ""
